---
title: "Homework 3: Discriminant Analysis"
subtitle: "S&DS 5360 | Multivariate Statistics"
author: "Brandon Tran (brandon.tran@yale.edu)"
date: today
date-format: "MMMM DD, YYYY"
format: pdf 
---



```{r, include = FALSE}
library(MASS)
library(tidyverse)
library(knitr)
library(kableExtra)
library(GGally)
library(heplots)
library(haven)
library(float)
library(klaR)
source("https://raw.githubusercontent.com/jreuning/sds363_code/refs/heads/main/discrim.r.txt")
```



# Setup & Data Overview
We continue with our analysis of the 2024--2025 Healthy Minds Study (HMS)
student survey, this time performing discriminant analysis. We will investigate
whether we can use the same psychological health scores we defined in Homework
2 to determine whether a student belongs to one of the following GSM identities:

1. Cis-Heterosexual
2. Cis-LGBQ+ (lesbian, gay, bisexual, or queer, but not TGNC)
3. Transgender and Gender Nonconforming (TGNC), including those who also identify as LGBQ+

```{r, include = FALSE, cache = TRUE}
hms_data <- read_sav("../data/hms_data.sav", encoding = 'latin1')

df <- hms_data %>%

    mutate(across(c(gender_male, gender_female, gender_trans, gender_queer, 
                    gender_nonbin, gender_selfID,
                    sexual_h, sexual_l, sexual_g, sexual_bi, sexual_queer, 
                    sexual_quest, sexual_pan, sexual_asexual), 
                    ~replace_na(., 0))) %>%

    mutate(

        trans = (gender_trans == 1 | gender_queer == 1 | gender_nonbin == 1 |
                gender_selfID == 1),

        queer = (sexual_l == 1 | sexual_g == 1 | sexual_bi == 1 | sexual_queer == 1 | 
                sexual_quest == 1 | sexual_pan == 1 | sexual_asexual == 1),
        
        identity_group = case_when(
            trans ~ "TGNC",
            !trans & queer ~ "Cis-LGBQ+",        
            !trans & !queer & sexual_h == 1 ~ "Cis-Het",
            TRUE ~ NA_character_
        )
    ) %>%

    select(identity_group, deprawsc, anx_score, lonesc, flourish) %>%
    
    drop_na() %>%
    mutate(identity_group = as.factor(identity_group))
```

Per the grader's feedback, we omit the code necessary to load and preprocess
the data. However, we provide summary Table 1 contextualize the subsequent
analyses:

```{r, echo = FALSE, cache = TRUE}
summary_stats <- df %>%
  group_by(identity_group) %>%
  summarise(
    n = format(n(), big.mark = ","), 
    "Depression Mean" = mean(deprawsc),
    "Depression SD" = sd(deprawsc),
    "Anxiety Mean" = mean(anx_score),
    "Anxiety SD" = sd(anx_score),
    "Loneliness Mean" = mean(lonesc),
    "Loneliness SD" = sd(lonesc),
    "Flourishing Mean" = mean(flourish),
    "Flourishing SD" = sd(flourish)
  ) %>%
  mutate(across(where(is.numeric), \(x) format(round(x, 2), nsmall = 2))) %>%
  pivot_longer(cols = -identity_group, names_to = "Statistic", values_to = "Value") %>%
  pivot_wider(names_from = identity_group, values_from = Value)

summary_stats %>%
  kbl(caption = "Descriptive Statistics by Identity Group",
      booktabs = T,
      format = "latex") %>%
  kable_styling(latex_options = c("striped", "HOLD_pos")) %>%
  pack_rows("Sample Size", 1, 1) %>%
  pack_rows("Depression (PHQ-9)", 2, 3) %>%
  pack_rows("Anxiety (GAD-7)", 4, 5) %>%
  pack_rows("Loneliness (UCLA)", 6, 7) %>%
  pack_rows("Flourishing", 8, 9)
```

We visually observe a difference across psychological health score means for
each group and note the significant sample size. We feel comfortable proceeding
with our analysis.



# Part 1 | Evaluating Assumptions
We begin by visualizing our data via a matrix (pairs) plot:
```{r}
set.seed(4747)
df_sample <- df %>% sample_n(2000)

ggpairs(
    df_sample,
    columns = c('deprawsc', 'anx_score', 'lonesc', 'flourish'),
    aes(color = identity_group, alpha = 0.5),
    upper = list(continuous = wrap('cor', size = 3)),
    title = 'Matrix Plot of Psychological Health Variables by Identity'
)
```

We see strong **separaton of means** for flourishing (`flourish`) and depression
(`deprawsc`). In comparison, loneliness (`lonesc`) feataures a lot of overlap
and likely will not be a strong predictor in our model.

Regarding **normality,** we first note a slight right skew for depression
(`deprawsc`) and anxiety (`anx_score`), particularly for the Cis-Het group. The
other groups appear more symmetrical. Loneliness (`lonesc`) is visibly
multimodal, violating normality (likely due to the LIkert scale structure of the
question), and potentially reducing the accuracy of the discriminant function.
Finally, flourishing (`flourish`) is left-skewed. We formally analyze
multivariate normality via chi-square quantile plots.

```{r}
par(mfrow=c(1,3))
cqplot(df[df$identity_group == "Cis-Het",
    c("deprawsc", "anx_score", "lonesc", "flourish")], main = "Cis-Het Q-Q Plot")
cqplot(df[df$identity_group == "Cis-LGBQ+",
    c("deprawsc", "anx_score", "lonesc", "flourish")], main = "Cis-LGBQ+ Q-Q Plot")
cqplot(df[df$identity_group == "TGNC",
    c("deprawsc", "anx_score", "lonesc", "flourish")], main = "TGNC Q-Q Plot")
```

Our Chi-Square Quantile plots show heavy deviations from the diagonal in the
upper tails for all groups, though less so for `TGNC`. This indicates that our
data is not multivariate normal, likely due to the skew caused by Likert scale
rankings. We consider the same transformations as in Homework 2:

- Square root of depression
- Square root of anxiety
- Square (quadratic) of flourish

However, these and similar transformations do not yield improved multivariate
normality. We supporess the code for the ease of the grader but present the
corresponding Q-Q plots for the transformed data:

```{r, echo = FALSE}
df_transform <- df %>%
    mutate(
        deprawsc_sqrt = sqrt(deprawsc + 0.5),
        anx_sqrt = sqrt(anx_score + 0.5),
        flourish_sq = flourish^2
    ) %>%
    
    select(identity_group, flourish_sq, deprawsc_sqrt, anx_sqrt, lonesc)
```

```{r, echo = FALSE}
par(mfrow=c(1,3))
cqplot(df_transform[df_transform$identity_group == "Cis-Het",
    c("deprawsc_sqrt", "anx_sqrt", "lonesc", "flourish_sq")], main = "Cis-Het Transformed")
cqplot(df_transform[df_transform$identity_group == "Cis-LGBQ+",
    c("deprawsc_sqrt", "anx_sqrt", "lonesc", "flourish_sq")], main = "Cis-LGBQ+ Transformed")
cqplot(df_transform[df_transform$identity_group == "TGNC",
    c("deprawsc_sqrt", "anx_sqrt", "lonesc", "flourish_sq")], main = "TGNC Transformed")
```

While the Chi-Square Quantile plots indicate a violation of multivariate normality,
we recall that discriminant analysis is traditionally robust to such violations
in large samples like ours. The non-normality is an inherent feature of the
floor effects of the Likert scale ratings, so while transformations were
considered, we ultimately choose to proceed with the raw variables to preserve
the interpretability of the results. The violation suggests that QDA could be
tested as a potentially more flexible alternative to LDA, but ultimately, we
feel comfortable proceeding with LDA.

Finally, we check for **equal covariance**. The matrix plot shows that
anxiety versus depression maintains fairly equal correlation across all groups
($0.747$, $0.710$, $0.694$). This trend is reasonably consistent among other
predictor pairs; however, flourishing versus depression, for example, features
wider gaps in correlation ($0.472, 0.448, 0.356$). These still feel reasoably
equivalent, but for completeness, we formally test for equal covariance via
Box's M-test.

```{r}
boxM(df[, c("deprawsc", "anx_score", "lonesc", "flourish")], df$identity_group)
```

As expected, given our large sample size of $66,333$ (aggregate count by group
can be seen in Table 1), leads our Box's M test to reject the null hypothesis,
indicating a difference in covariances across groups. We instead analyze the
practical difference in covariance structures.

```{r}
vars <- c("deprawsc", "anx_score", "lonesc", "flourish")

get_ratio <- function(m1, m2) {
  r <- m1 / m2
  r[abs(r) < 1] <- 1 / r[abs(r) < 1]
  return(round(r, 1))
}

# Calculate Covariance Matrices
cov_cis  <- cov(df[df$identity_group == "Cis-Het", vars])
cov_lgbq <- cov(df[df$identity_group == "Cis-LGBQ+", vars])
cov_tgnc <- cov(df[df$identity_group == "TGNC", vars])

# Calculate Ratios
rat_tgnc_cis  <- get_ratio(cov_tgnc, cov_cis)
rat_lgbq_cis  <- get_ratio(cov_lgbq, cov_cis)
rat_tgnc_lgbq <- get_ratio(cov_tgnc, cov_lgbq)
```

```{r, echo = FALSE}
mat1 <- matrix(c(1.2, 1.0, 1.1, 1.4,
                 1.0, 1.0, 1.3, 1.1,
                 1.1, 1.3, 1.1, 1.1,
                 1.4, 1.1, 1.1, 1.0), nrow=4, byrow=TRUE)

mat2 <- matrix(c(1.1, 1.0, 1.1, 1.2,
                 1.0, 1.1, 1.2, 1.0,
                 1.1, 1.2, 1.0, 1.0,
                 1.2, 1.0, 1.0, 1.1), nrow=4, byrow=TRUE)

mat3 <- matrix(c(1.1, 1.0, 1.1, 1.2,
                 1.0, 1.0, 1.1, 1.1,
                 1.1, 1.1, 1.1, 1.1,
                 1.2, 1.1, 1.1, 1.1), nrow=4, byrow=TRUE)

# 2. Combine into a single Data Frame
cov_ratios <- rbind(mat1, mat2, mat3) %>% as.data.frame()
colnames(cov_ratios) <- c("Depression", "Anxiety", "Loneliness", "Flourishing")

# 3. Add the Labels as a proper Column (Metric)
cov_ratios$Metric <- rep(c("Depression", "Anxiety", "Loneliness", "Flourishing"), 3)

# 4. Reorder columns so 'Metric' is first
cov_ratios <- cov_ratios %>% select(Metric, everything())

# 5. Generate the Table
cov_ratios %>%
  kbl(caption = "Ratio of Covariance Matrix Elements Between Groups", 
      align = "c",
      row.names = FALSE, # Turn off default row names
      format = "latex",
      booktabs = T) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  
  # Grouping
  pack_rows("Comparison 1: TGNC vs. Cis-Het", 1, 4, label_row_css = "background-color: #f0f0f0; color: #333;") %>%
  pack_rows("Comparison 2: Cis-LGBQ+ vs. Cis-Het", 5, 8, label_row_css = "background-color: #f0f0f0; color: #333;") %>%
  pack_rows("Comparison 3: TGNC vs. Cis-LGBQ+", 9, 12, label_row_css = "background-color: #f0f0f0; color: #333;")
```

Following the rule where if all ratios of covariance matrix elements between
groups are less than four, the covariance matrices are similar enough, we feel
comfortable proceeding with linear discrimant analysis, as all of our ratios fall
between $1.0$ and $1.4$.



# Part 2 | Discriminant Analysis
With our assumptions somewhat satisfied in Part 1, we proceed with both Linear
Discriminant Analysis and Quadratic Discriminant Analysis. We will compare
their classification accuracy and proceed with analysis via the outperforming
method.

```{r}
lda_cv <- lda(
    identity_group ~ deprawsc + anx_score + lonesc + flourish,
    data = df,
    prior = c(1/3, 1/3, 1/3),
    CV = TRUE)

qda_cv <- qda(
    identity_group ~ deprawsc + anx_score + lonesc + flourish,
    data = df,
    prior = c(1/3, 1/3, 1/3),
    CV = TRUE)

lda_cv_acc <- round(sum(diag(prop.table(table(df$identity_group, lda_cv$class)))), 4)
qda_cv_acc <- round(sum(diag(prop.table(table(df$identity_group, qda_cv$class)))), 4)

cat("LDA CV Accuracy:", lda_cv_acc, "\nQDA CV Accuracy:", qda_cv_acc)
```

We see that the cross-validated accuracy of QDA is only $0.0033$ higher than
that of LDA. Therefore, to preserve interpretability (as discussed in Part 1)
and to adhere with the principle of parsimony, we proceed with LDA on our
raw (untransformed) variables.

Note that we compared LDA versus QDA as full models (inclusive of all
discriminants). Now that we have elected to use LDA, we proceed with Stepwise
Discriminant Analysis to determine whether there are any redundant
discriminators we should drop. Note we proceed with $10$-fold validation due
to our large sample size.

```{r, }
df_clean <- df %>% drop_na()

X <- df_clean[, c('deprawsc', 'anx_score', 'lonesc', 'flourish')]
y <- df_clean$identity_group

step_model <- stepclass(
    X,
    y,
    method = "lda",
    direction = "both",
    prior = c(1/3, 1/3, 1/3),
    improvement = 0.01,
    fold = 10,
    trace = FALSE)

step_model
```

With equal priors and a low improvement threshold, our Stepwise Discriminant
Analysis proceeds with solely `deprawsc` as the discriminator, yielding essentially
the same correctness rate as our full model ($0.5172$ versus $0.5167$, respectively).
This is unsurprising, as we saw that the discriminators themselves exhibit
multicollinearity. Thus, one discriminator alone may already provide a bulk of
the signal used to classify each individual. However, per the recommendation in
the assignment text, we proceed with the full model (all four discriminators).
This will allow us to conduct a more granular deep dive into the multivariate
psychological health profiles of each group in later analyses. The model summary
is produced below:

```{r}
lda_fit <- lda(identity_group ~ ., data = df, prior = c(1/3, 1/3, 1/3)); lda_fit
```



# Part 3 | Multivariate Group Means
To determine whether the multivariate group means are different, we perform the
multivariate Wilks' Lambda Test:

```{r}
df_manova <- manova(as.matrix(df[, vars]) ~ df$identity_group)
summary(df_manova, test = "Wilks")
```

Our Wilks' Lambda is $0.9191$, indicating that approximately $8.1\%$ of the
total variance in the combined mental health scores is explained solely by the
student's identity group. With an extremely small $p$-value, we can reject the
null hypothesis and conclude that mean vectors are not the same across groups.

We further present the univariate analyses (equivalent to `summary.aov` output)
to better understand the relevance of each discriminator in Table 3.

```{r, echo = FALSE}
manova_results <- data.frame(
  Test = c("Multivariate (Wilks)", "Depression (PHQ-9)", "Anxiety (GAD-7)", "Loneliness (UCLA)", "Flourishing"),
  Statistic = c("0.9191", "---", "---", "---", "---"),
  F_Value = c(714.34, 2342.8, 1766.5, 1645.5, 1432.4),
  p_value = c("< 2.2e-16", "< 2.2e-16", "< 2.2e-16", "< 2.2e-16", "< 2.2e-16")
)

manova_results %>%
  kbl(caption = "MANOVA and Univariate ANOVA Results for Identity Groups",
      booktabs = TRUE,
      format = "latex",
      col.names = c("Analysis Level", "Wilks' Lambda", "F-Statistic", "p-value"),
      align = "lccc") %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  pack_rows("Multivariate Test", 1, 1) %>%
  pack_rows("Univariate Response Tests", 2, 5)
```

We see that each predictor independently is a statistically significant 
discriminator of group differences. Given our sample size and earlier
discussion on collinearity, this is unsurprising. However, note that the $F$-
statistic for depression is much larger than the other discriminators. This
confirms the stepwise decision earlier to retain only depression in the model.

Ultimately, we are satisfied with our Wilks' Lambda value of $0.9191$. Explaining
$8.1\%$ of the total variance across groups is generally considered a medium
effect in the social sciences.



# Part 4 | Signifiance of Discriminant Functions
Recall the proportion of trace metrics we computed in Part 2:
```{r}
lda_fit
```

Since we have $3$ groups, we have $2$ discriminant functions. Our first
discriminant function (`LD1`) accounts for $98.28\%$ of the relative
discriminating power---almost all of the necessary discriminatory signal in our
predictor variables. `LD2`, on the other hand, captures only the remaining
$1.72\%$, though it is statistically significant (likely due to sample size).
This suggests that the psychological differences between each group are largely
one-dimensional.



# Part 5 | Classification
Recall that we performed both regular and $10$-fold cross validation to
determine whether to proceed with LDA or QDA in Part 2.

```{r}
lda_regular_acc <- round(sum(diag(prop.table(table(df$identity_group, predict(lda_fit)$class)))), 4)
acc_comp <- data.frame(
  Type = c("Regular Accuracy", "Cross-Validated Accuracy"),
  Value = c(lda_regular_acc, lda_cv_acc)
)

acc_comp
```

Our regular accuracy and $10$-fold cross-validated accuracy are nearly identical,
with a difference of $0.0001$. This indicates that our model is stable. Now,
we are curious whether there are certain areas where the model does particularly
well (or not). For this, we generate a confusion matrix and append recall
(accuracy percentages):

```{r}
conf_matrix <- table(Actual = df$identity_group, Predicted = lda_cv$class)
```

```{r, echo = FALSE}
conf_data <- data.frame(
  Actual_Group = c("Cis-Het", "Cis-LGBQ+", "TGNC"),
  Pred_CisHet = c(28363, 7040, 1498),
  Pred_LGBQ = c(6331, 3106, 852),
  Pred_TGNC = c(10097, 6241, 2805)
)

# 2. Calculate Recall % for each row
conf_data$Total <- rowSums(conf_data[, 2:4])
conf_data$Recall <- round(c(28363, 3106, 2805) / conf_data$Total * 100, 2)

# 3. Generate the Kable table
conf_data[, -4] %>%
  kbl(caption = "Cross-Validated Confusion Matrix for Identity Groups",
      booktabs = TRUE,
      format = "latex",
      col.names = c("Actual Group", "Pred: Cis-Het", "Pred: Cis-LGBQ+", "Pred: TGNC", "Recall (%)"),
      align = "lcccc") %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Predicted Group Membership" = 3, " " = 1))
```

Though our overall cross-validated accuracy is $\approx 52\%$, as illustrated in
Table 4 the model is most effective at predicting `Cis-Het` and `TGNC`, with
recalls of $63.3\%$ and $54.4\%$, accordingly. However, the `Cis-LGBQ+` group
has a low recall of $19.0\%$. Most of these students are misclassified as one
of the other two groups, illustrating that their mental health data doesn't
form a unique cluster.
